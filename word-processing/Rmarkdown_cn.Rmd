---
title: 'From Nonparametrics to Machine Learning'
author: "Zhentao Shi"
date: "April 15, 2020"


documentclass: ctexart # for Chinese characters
output:
  rticles::ctex:

description: "nothing"
geometry: margin=1in

bibliography: book.bib
biblio-style: apalike
link-citations: yes
fontsize: 11pt
urlcolor: blue

header-includes:
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usetikzlibrary{shapes.geometric, arrows,chains,matrix,scopes}
  - \usepackage{caption}
  - \usepackage{setspace}
  - \singlespacing
---


# From Nonparametrics to Machine Learning

机器学习迅速成长成为一个大领域，有许多日常应用的场景。
An authoritative reference is @friedman2001elements,
written at the entry-year postgraduate level.
The ideas in machine learning are general and applicable to economic investigation.
@athey2018impact discusses the impact of machine learning techniques
to economic analysis.
@mullainathan2017machine survey a few new commonly used methods and
demonstrate them in a real example.
@taddy2018technological introduces new technology *artificial intelligence*
and the implication of the underlying economic modeling.


The two broad classes of machine learning methods are *supervised learning* and *unsupervised learning*.
Roughly speaking, the former is about the connection between $X$ and $Y$, while the latter
is only about $X$. Instances of the former are various regression
and classification methods; those of the latter are density estimation,
principal component analysis, and clustering.
These examples are all familiar econometric problems.

From an econometrician's view, supervised machine learning is a set of data fitting procedures that focus on out-of-sample prediction.
The simplest illustration is in the regression context.
We repeat a scientific experiment for $n$ times, and we harvest a dataset $(y_i, x_i)_{i=1}^n$.
What would be the best way to predict $y_{n+1}$ from the same experiment if we know $x_{n+1}$?

Machine learning is a paradigm shift against conventional statistics.
When a statistician propose a new estimator, the standard practice is to pursue
three desirable properties one after another.
We first establish its consistency, which is seen as the bottom line.
Given consistency, we want to show its asymptotic distribution. Ideally, the asymptotic
distribution is normal. Asymptotic normality is desirable as it holds for many regular estimators
and the inferential procedure is familiar to applied researchers.
Furthermore, for an asymptotically normal estimator, we want to
show efficiency, an optimality property. An efficient estimator achieves the smallest asymptotic variance
in a class of asymptotically normal estimators.

In addition, econometrician also cares about
model identification and economic interpretation of the empirical results.
Econometrics workflow interacts the data at hand and the model of interest. At the population level,
we think about the problem of identification. Once the parameter of interest is identified,
then we can proceed to parameter estimation and inference.
Finally, we interpret the results and hopefully they shed light on economics.

<!-- \begin{figure} -->
<!-- \centering -->
<!-- \begin{tikzpicture}[node distance=5mm] -->
<!--     %\draw[help lines] (0,0) grid (9,7); -->

<!-- % Nodes -->
<!--     \node [draw, ellipse] (a) at (2,6) {Model}; -->
<!--     \node [draw, ellipse] (b) at (8,6) {abstract data}; -->
<!--     \node [color=black!40](c) at (1,5) {Population}; -->
<!--     \node (d) at (5,4) {Estimation}; -->
<!--     \node [color=black!40](e) at (1,2) {Sample}; -->
<!--     \node (f) at (5,2) {Inference}; -->
<!--     \node (g) at (5,0) {Interpretation}; -->
<!--     \node [color=black!40](h) at (1,0) {Economics}; -->

<!-- % Arrows   -->
<!--     \draw[-latex] (a) to[bend right=10] node[above, yshift=2mm] {Identification}  (b); -->
<!--     \draw[-latex] (b) to[bend right=10]  (a); -->
<!--     \draw[-latex] (a) to[bend left=0]  (d); -->
<!--     \draw[-latex] (b) to[bend right=0]  (d); -->
<!--     \draw[-latex] (d) to[bend right=0]  (f); -->
<!--     \draw[-latex] (f) to[bend right=0]  (g); -->

<!-- % Lines -->
<!--     \draw[color=black!40,very thick] (0,1) -- (2,1); -->
<!--     \draw[color=black!40,very thick] (0,4) -- (2,4); -->
<!-- \end{tikzpicture} -->
<!-- \caption{Econometrics workflow} -->
<!-- \end{figure} -->



## Data Splitting


<!-- \begin{figure} -->
<!-- \centering -->
<!-- \begin{tikzpicture}[node distance = 10 mm, thick, scale=1, transform shape] -->
<!-- % Nodes -->
<!--   \node[ellipse, draw] -->
<!--     (a) [label=above left:$data$]{Training Data}; -->
<!--   \node[ellipse, draw, right = of a] -->
<!--     (b) {Validation Data}; -->
<!--   \node[ellipse, draw=red,fill=black!0, right = of b, text width=3cm, align=center] -->
<!--     (c) {(Out of Sample) Testing Data}; -->
<!--   \node[ellipse, draw=blue,fill=black!0, below right = 20 mm of a] -->
<!--     (d) {Fitted model} ; -->
<!--   \node[ellipse, draw=blue,fill=black!0, right= 10 mm of d, text width=3.5cm, align=center] -->
<!--     (e) {Best tuning Parameter (Model)}; -->
<!--   \node[ellipse, draw, left = 20 mm of d, loosely dashed] -->
<!--     (f) -->
<!--     [label = below: \textcolor{black!40}{Many Sets of Tuning Parameters}] -->
<!--     {Model}; -->

<!-- % Arrows -->
<!--     \draw [->, black] (a) -- (d); -->
<!--     \draw [->, blue] (d) -- (b); -->
<!--     \draw [->, blue] (b) -- (e); -->
<!--     \draw [->, blue] (e) -- (c); -->
<!--     \draw [->, black, loosely dashed] (f) -- (d); -->

<!-- % Caption     -->
<!--   \node[below = of d] { -->
<!--         \begin{tabular}{l} -->
<!--             $\bullet$ Data splitting can be done by cross validation \\ -->
<!--             $\bullet$ A data driven approach for feature selection -->
<!--         \end{tabular}}; -->
<!-- \end{tikzpicture} -->
<!-- \caption{Learning workflow} -->
<!-- \end{figure} -->


The workflow of machine learning methods is quite different from econometrics. The main purpose is often prediction instead of interpretation.
They use some "off-the-shelf" generic learning methods, and
the models are measured by their performance in prediction.
In order to avoid overfitting it is essential to tune at least a few tuning parameters.



More methods are available if prediction of the response variables is the sole purpose of the regression.
An intuitive one is called *stagewise forward selection*.
We start from an empty model. Given many candidate $x_j$, in each round we add the regressor that can
produce the biggest $R^2$. This method is similar to the idea of $L_2$ componentwise boosting,
which does not adjust the coefficients fitted earlier.

In reality, we have a sample $(y_i, x_i)$ of $n$ observations, and we estimate $\beta$ by the OLS estimator $\hat{\beta} = (X'X)^{-1}X'y$.
The expectation of the SSR is
$$
E\left[ \frac{1}{n} \sum_{i=1}^n (y_i - x_i' \hat{\beta})^2  \right]
= \frac{1}{n}  E\left[ e'(I_n - X(X'X)^{-1}X )e  \right]=  \frac{\sigma^2}{n}(n-p) = \sigma^2\left( 1 - \frac{p}{n} \right)
< \sigma^2
$$
Asymptotically, if $p/n \to 0$, the two risks converge. Otherwise if $p/n \to c$, the expected SSR is strictly
smaller than the minimal population risk. The model is overfitted.


## References